{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "168b6897",
   "metadata": {},
   "source": [
    "# Feature Engineering and Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25decaf4",
   "metadata": {},
   "source": [
    "## Goals\n",
    "In this lab we:\n",
    "- explore feature engineering and polynomial regression which allows us to use the machinery of linear regression to fit very complicated, even very non-linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00afde7f",
   "metadata": {},
   "source": [
    "## Tools\n",
    "We utilize the function developed in previous labs as well as `matplotlib` and NumPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85bc62af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_multi import zscore_normalize_features, run_gradient_descent_feng\n",
    "np.set_printoptions(precision=2)  # reduced display precision on numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936f0ac",
   "metadata": {},
   "source": [
    "<a name='FeatureEng'></a>\n",
    "# Feature Engineering and Polynomial Regression Overview\n",
    "\n",
    "Out of the box, linear regression provides a means of building models of the form:\n",
    "\n",
    "$$f_{\\mathbf{w},b} = w_0x_0 + w_1x_1+ ... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "\n",
    "But what if our features/data are non-linear or are a combination of features? Housing prices tend to correlate linearly with living area but the trend penalizes very small or very large houses, resulting in unbecoming curves. The question then is, how can we use the machinery of linear regression to trace a proper line-of-best-fit? Recall, the 'machinery' we have is the ability to modify the parameters $\\mathbf{w}$, $\\mathbf{b}$ in (1) to 'fit' the equation to the training data. However, no amount of adjusting of $\\mathbf{w}$ and $\\mathbf{b}$ in (1) will fit a non-linear curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c063f97a",
   "metadata": {},
   "source": [
    "<a name='PolynomialFeatures'></a>\n",
    "## Polynomial Features\n",
    "\n",
    "Above, we were considering a scenario where the data was non-linear. Let's try using what we know so far to fit a non-linear curve. We'll start with a simple quadratic function: $y = 1+x^2$\n",
    "\n",
    "You are familiar with all the routines we are using. They are available in the `lab_utils.py` file for review. We will use [`np.c_[..]`](https://numpy.org/doc/stable/reference/generated/numpy.c_.html) which is a NumPy routine to concatenate along the column boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a48fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target data\n",
    "x = np.arange(0, 20, 1)\n",
    "y = 1 + x**2\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "model_w, model_b = run_gradient_descent_feng(X, y, iterations=1000, alpha=1e-2)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"no feature engineering\")\n",
    "plt.plot(x,X@model_w + model_b, label=\"Predicted Value\");  plt.xlabel(\"X\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29eb228",
   "metadata": {},
   "source": [
    "<img src=\"./images/img01.png\" style=\"width:500px\"/>\n",
    "\n",
    "Well, as expected, not a great fit. What is needed is something like $y= w_0x_0^2 + b$, or a **polynomial feature**.\n",
    "To accomplish this, you can modify the *input data* to *engineer* the needed features. If you swap the original data with a version that squares the $x$ value, then you can achieve $y= w_0x_0^2 + b$. Let's try it. Swap `X` for `X**2` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cadb65cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target data\n",
    "x = np.arange(0, 20, 1)\n",
    "y = 1 + x**2\n",
    "\n",
    "# Engineer features \n",
    "X = x**2      #<-- added engineered feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd195cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(-1, 1)  #X should be a 2-D Matrix\n",
    "model_w, model_b = run_gradient_descent_feng(X, y, iterations=10000, alpha=1e-5)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"Added x**2 feature\")\n",
    "plt.plot(x, np.dot(X,model_w) + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610292f9",
   "metadata": {},
   "source": [
    "<img src=\"./images/img02.png\" style=\"width:1200px\"/>\n",
    "\n",
    "Great! A near perfect fit. Notice the values of $\\mathbf{w}$ and $b$ printed right above the graph: `w,b found by gradient descent: w: [1.], b: 0.0490`. Gradient descent modified our initial values of $\\mathbf{w},b $ to be (1.0,0.049) or a model of $y=1*x_0^2+0.049$, very close to our target of $y=1*x_0^2+1$. Running it longer would produce a better match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc57636",
   "metadata": {},
   "source": [
    "### Selecting Features\n",
    "<a name='GDF'></a>\n",
    "Above, we knew that an $x^2$ term was required, but this is hardly ever the case. It is possible to add a variety of potential features to try and find the most useful ones. For example, what if we had instead tried : $y=w_0x_0 + w_1x_1^2 + w_2x_2^3+b$ ? \n",
    "\n",
    "Run the next cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "447f2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target data\n",
    "x = np.arange(0, 20, 1)\n",
    "y = x**2\n",
    "\n",
    "# engineer features .\n",
    "X = np.c_[x, x**2, x**3]   #<-- added engineered feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9a0c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w, model_b = run_gradient_descent_feng(X, y, iterations=10000, alpha=1e-7)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"x, x**2, x**3 features\")\n",
    "plt.plot(x, X@model_w + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7500dabd",
   "metadata": {},
   "source": [
    "<img src=\"./images/img03.png\" style=\"width:1500px\"/>\n",
    "\n",
    "Note the values of $\\mathbf{w}$, `[0.08 0.54 0.03]` and $b$, `0.0106`. This implies the model after fitting/training is:\n",
    "$$ 0.08x + 0.54x^2 + 0.03x^3 + 0.0106 $$\n",
    "Gradient descent has emphasized the $x^2$ term by increasing the $w_1$ term relative to the other weighting parameters to determine the best fit.  Running it for longer would likely continue to reduce the impact of the other terms.\n",
    ">Gradient descent is picking the 'correct' features for us by emphasizing their associated parameters\n",
    "\n",
    "Let's review this idea:\n",
    "- Intially, the features were re-scaled so they are comparable to each other\n",
    "- smaller weight values imply less relevant/correct features. In extreme cases, the weight becomes zero or very close to zero when the associated feature is not useful in fitting the model to the data.\n",
    "- what we see above is that after fitting, the weight associated with the $x^2$ feature is significantly larger than the weights associated with $x$ or $x^3$ since it is the most useful in fitting the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e21520",
   "metadata": {},
   "source": [
    "### An Alternate View\n",
    "Above, polynomial features were chosen based on how well they matched the target data. Another way to think about this is to note that we are still using linear regression once we have modified the features to create new ones. Essentially, the best features will be linear relative to the target. Let's demonstrate this with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "550d41e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target data\n",
    "x = np.arange(0, 20, 1)\n",
    "y = x**2\n",
    "\n",
    "# engineer features .\n",
    "X = np.c_[x, x**2, x**3]   #<-- added engineered feature\n",
    "X_features = ['x','x^2','x^3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aa6a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1, 3, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X[:,i],y)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6cf6fe",
   "metadata": {},
   "source": [
    "<img src=\"./images/img04.png\" style=\"width:800px\"/>\n",
    "\n",
    "Evidently, the term $x^2$ is linear against the target value $y$. We can therefore use simple linear regression with the modified feature $x^2$ to generate a model that can predict this data fairly accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e5c4d",
   "metadata": {},
   "source": [
    "### Scaling features\n",
    "\n",
    "As described in the previous lab, if the data set has features with significantly different scales, we should apply feature scaling to speed up gradient descent. In the example above, there is $x$, $x^2$ and $x^3$ which will naturally have very different scales. Let's apply Z-score normalization to our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6965f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak to Peak range by column in Raw        X:[  19  361 6859]\n",
      "Peak to Peak range by column in Normalized X:[3.3  3.18 3.28]\n"
     ]
    }
   ],
   "source": [
    "# create target data\n",
    "x = np.arange(0,20,1)\n",
    "X = np.c_[x, x**2, x**3]\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X,axis=0)}\")\n",
    "\n",
    "# add mean_normalization \n",
    "X = zscore_normalize_features(X)     \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X,axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a0d456",
   "metadata": {},
   "source": [
    "Now we can try again with a more aggressive value of alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda3943",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,20,1)\n",
    "y = x**2\n",
    "\n",
    "X = np.c_[x, x**2, x**3]\n",
    "X = zscore_normalize_features(X) \n",
    "\n",
    "model_w, model_b = run_gradient_descent_feng(X, y, iterations=100000, alpha=1e-1)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"Normalized x x**2, x**3 feature\")\n",
    "plt.plot(x,X@model_w + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d59cbf",
   "metadata": {},
   "source": [
    "<img src=\"./images/img05.png\" style=\"width:1200px\"/>\n",
    "\n",
    "Feature scaling allows gradient descent to converge much faster.   \n",
    "Note again the values of $\\mathbf{w}$. The $w_1$ term, which is the $x^2$ term is the most emphasized. Gradient descent has all but eliminated the other terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eef3fa",
   "metadata": {},
   "source": [
    "### Complex Functions\n",
    "With feature engineering, even quite complex functions can be modeled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ecefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,20,1)\n",
    "y = np.cos(x/2)\n",
    "\n",
    "X = np.c_[x, x**2, x**3,x**4, x**5, x**6, x**7, x**8, x**9, x**10, x**11, x**12, x**13]\n",
    "X = zscore_normalize_features(X) \n",
    "\n",
    "model_w, model_b = run_gradient_descent_feng(X, y, iterations=1000000, alpha=1e-1)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"Normalized x, x**2, x**3 feature\")\n",
    "plt.plot(x,X@model_w + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea865a91-ca3b-42ff-a632-cc15524ef66b",
   "metadata": {},
   "source": [
    "<img src=\"./images/img06.png\" style=\"width:1200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0926d",
   "metadata": {},
   "source": [
    "\n",
    "## Congratulations!\n",
    "In this lab we:\n",
    "- learned how linear regression can model complex, even highly non-linear functions using feature engineering\n",
    "- recognized that it is important to apply feature scaling when doing feature engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
